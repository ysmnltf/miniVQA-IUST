{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pK80v3DcjRIf"
      },
      "source": [
        "In this notebook, a **VQA** model is implemented using **PyTorch** library.\n",
        "\n",
        "- Question features are extracted using\n",
        "  - **Word2Vec or FastText Embeddings**\n",
        "  - **LSTM layers**\n",
        "- Image features are available in the dataset.\n",
        "- The question and image features are fused with\n",
        "  - **Concatenation**\n",
        "- The correct answer is predicted with a Dense layer.\n",
        "\n",
        "**Best Validation Accuracy: 0.763**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxtUB_kckUqZ"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6zl28OnjQKi"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "import pandas as pd\n",
        "import torch\n",
        "import pickle\n",
        "from torch import nn\n",
        "import torchtext\n",
        "import os\n",
        "import collections\n",
        "import numpy as np\n",
        "import json\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISGqJ9XIkYHh"
      },
      "source": [
        "# Loading data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3VZ2ft6kgNo"
      },
      "source": [
        "## Connecting to drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wumPEQakZtC",
        "outputId": "b86b3404-0408-4ef3-8387-9eb6729d3220"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/gdrive/', force_remount=True)\n",
        "base_path = '/content/gdrive/My Drive/iust/miniVQA/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDK3Dcw8keeS"
      },
      "source": [
        "## Reading data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cO_eqsRLkj5Z"
      },
      "source": [
        "### Answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOymOM0lkd8T"
      },
      "outputs": [],
      "source": [
        "all_answers = [ 'surfboard', 'eating', 'cake', 'table', 'hat', 'giraffe', 'broccoli', 'woman', 'sunny', 'apple']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwQdFvy6lCHZ"
      },
      "source": [
        "### Image features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6MfcYvae3lB"
      },
      "outputs": [],
      "source": [
        "with open(base_path + 'image_features.pickle', 'rb') as f:\n",
        "    image_features = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HA8Z54A4kmeu"
      },
      "source": [
        "### Questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnXSQ4j8koLI"
      },
      "outputs": [],
      "source": [
        "with open(base_path + 'image_question.json', 'r') as f:\n",
        "  img_to_q_dict = json.load(f)\n",
        "  questions = []\n",
        "  for img_id, img_qs in img_to_q_dict.items():\n",
        "    for img_q in img_qs:\n",
        "      q_id, q_text = img_q\n",
        "      questions.append({\n",
        "        'q_id': q_id,\n",
        "        'q_text': q_text,\n",
        "        'img_id': img_id\n",
        "      })\n",
        "\n",
        "questions = sorted(questions, key= lambda q: q['q_id'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_KpoLVZlpYW"
      },
      "source": [
        "### Subsets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIGLCdxeljhV"
      },
      "outputs": [],
      "source": [
        "train_csv = pd.read_csv(base_path + 'train.csv', index_col=\"question_id\").sort_index()\n",
        "train_csv.head()\n",
        "\n",
        "train_csv[\"question_text\"] = [q[\"q_text\"] for q in questions if q['q_id'] in train_csv.index.values]\n",
        "train_csv[\"image_id\"] = [q[\"img_id\"] for q in questions if q['q_id'] in train_csv.index.values]\n",
        "\n",
        "\n",
        "train_q = train_csv[\"question_text\"].values.tolist()\n",
        "train_a = torch.from_numpy(train_csv[\"label\"].values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7yWaPox3TW8"
      },
      "outputs": [],
      "source": [
        "valid_csv = pd.read_csv(base_path + 'val.csv', index_col=\"question_id\").sort_index()\n",
        "valid_csv.head()\n",
        "\n",
        "valid_csv[\"question_text\"] = [q[\"q_text\"] for q in questions if q['q_id'] in valid_csv.index.values]\n",
        "valid_csv[\"image_id\"] = [q[\"img_id\"] for q in questions if q['q_id'] in valid_csv.index.values]\n",
        "\n",
        "\n",
        "valid_q = valid_csv[\"question_text\"].values.tolist()\n",
        "valid_a = torch.from_numpy(valid_csv[\"label\"].values)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79pWZK_24YMb"
      },
      "outputs": [],
      "source": [
        "test_csv = pd.read_csv(base_path + 'test.csv', index_col=\"question_id\").sort_index()\n",
        "test_csv.head()\n",
        "\n",
        "test_csv[\"question_text\"] = [q[\"q_text\"] for q in questions if q['q_id'] in test_csv.index.values]\n",
        "test_csv[\"image_id\"] = [q[\"img_id\"] for q in questions if q['q_id'] in test_csv.index.values]\n",
        "\n",
        "\n",
        "test_q = test_csv[\"question_text\"].values.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4ubtau6oVAe"
      },
      "source": [
        "# Create word embeddings layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GM3xeeLSoaCo"
      },
      "source": [
        "## Download model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKWqE04zzuxT"
      },
      "outputs": [],
      "source": [
        "# embedding_model_name = \"word2vec-google-news-300\"\n",
        "embedding_model_name = \"fasttext-wiki-news-subwords-300\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjJobTbXoYCK",
        "outputId": "84b29eaf-4727-4f3b-c8d5-c69a5059da8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 958.5/958.4MB downloaded\n"
          ]
        }
      ],
      "source": [
        "embedding_model = api.load(embedding_model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzNlqVSVoe6O"
      },
      "source": [
        "## Preprocess questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pd1UMUcQyOZc"
      },
      "outputs": [],
      "source": [
        "class WordEmbeddings(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        return self.embedding(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sR_D51yrojTX"
      },
      "outputs": [],
      "source": [
        "# Tokenize\n",
        "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
        "\n",
        "# Create embedding layer\n",
        "embed_size = len(embedding_model.get_vector('hello'))\n",
        "word_embeddings = WordEmbeddings(\n",
        "    vocab_size = len(embedding_model.index_to_key) + 1,\n",
        "    embed_dim = embed_size,\n",
        ")\n",
        "word_embeddings.embedding.weight.data[0] = torch.zeros(embed_size)\n",
        "word_embeddings.embedding.weight.data[1:] = torch.from_numpy(embedding_model.vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMimRA5TpAFv"
      },
      "outputs": [],
      "source": [
        "def encode(x):\n",
        "  return [embedding_model.get_index(token, default=-1) + 1 for token in tokenizer(x)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_e_dLF1pESz"
      },
      "outputs": [],
      "source": [
        "def padify(xs, l = 15):\n",
        "    encoded_x = [encode(x) for x in xs]\n",
        "    return torch.stack([torch.nn.functional.pad(torch.tensor(t),(0,l-len(t)),mode='constant',value=0) for t in encoded_x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rY1u5iQpmjW",
        "outputId": "9a8a08f9-7490-48ab-a07d-b7e2498234c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train q embeddings size: torch.Size([780, 15, 300])\n",
            "Train image features shape: torch.Size([780, 512])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-21-951933259558>:7: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  train_img = torch.Tensor([image_features[img_id] for img_id in train_csv[\"image_id\"].values])\n"
          ]
        }
      ],
      "source": [
        "# Apply on train\n",
        "train_q_embeddings = word_embeddings(\n",
        "    padify(train_q)\n",
        ")\n",
        "print('Train q embeddings size:', train_q_embeddings.shape)\n",
        "\n",
        "train_img = torch.Tensor([image_features[img_id] for img_id in train_csv[\"image_id\"].values])\n",
        "print('Train image features shape:', train_img.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yShPLEw13u8",
        "outputId": "fb2086cc-0dc6-47a8-bcfd-9689cadb5a30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Valid q embeddings size: torch.Size([110, 15, 300])\n",
            "Valid image features shape: torch.Size([110, 512])\n"
          ]
        }
      ],
      "source": [
        "# Apply on valid\n",
        "valid_q_embeddings = word_embeddings(\n",
        "    padify(valid_q)\n",
        ")\n",
        "print('Valid q embeddings size:', valid_q_embeddings.shape)\n",
        "\n",
        "valid_img = torch.Tensor([image_features[img_id] for img_id in valid_csv[\"image_id\"].values])\n",
        "print('Valid image features shape:', valid_img.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FddcgWpi8JIK",
        "outputId": "471f964f-2222-49bb-8e4c-b301b1e3cdc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test q embeddings size: torch.Size([110, 15, 300])\n",
            "Test image features shape: torch.Size([110, 512])\n"
          ]
        }
      ],
      "source": [
        "# Apply on test\n",
        "test_q_embeddings = word_embeddings(\n",
        "    padify(test_q)\n",
        ")\n",
        "print('Test q embeddings size:', test_q_embeddings.shape)\n",
        "\n",
        "test_img = torch.Tensor([image_features[img_id] for img_id in test_csv[\"image_id\"].values])\n",
        "print('Test image features shape:', test_img.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdLQgmjQqal0"
      },
      "source": [
        "## Create dataset and dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhvqUbRyqeuy"
      },
      "outputs": [],
      "source": [
        "train_dataset = torch.utils.data.TensorDataset(train_q_embeddings, train_img, train_a)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bsi5URq3ttA"
      },
      "outputs": [],
      "source": [
        "valid_dataset = torch.utils.data.TensorDataset(valid_q_embeddings, valid_img, valid_a)\n",
        "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWr5mrH3qgqT"
      },
      "source": [
        "# Build model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILxjYqGv3HiU",
        "outputId": "69096a52-f0c6-41d2-d17f-fce5e246d5ec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<function gc.collect(generation=2)>"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# empty memory\n",
        "del embedding_model\n",
        "del image_features\n",
        "del questions\n",
        "import gc\n",
        "gc.collect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqRV6gePqij9"
      },
      "outputs": [],
      "source": [
        "class MiniVQA(nn.Module):\n",
        "    def __init__(self, lstm_input_size, linear_features_size):\n",
        "        super(type(self), self).__init__()\n",
        "        self.lstms = nn.LSTM(lstm_input_size, linear_features_size, num_layers=1)\n",
        "\n",
        "        self.linears = nn.Sequential(\n",
        "            nn.Linear(linear_features_size * 15 + 512, 128),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128, 10),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, text, image):\n",
        "        text_features = self.lstms(text)[0]\n",
        "        text_features= torch.flatten(text_features, start_dim=1)\n",
        "        features = torch.cat([text_features, image], dim=1)\n",
        "        logits = self.linears(features)\n",
        "        return nn.functional.softmax(logits, dim=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9CqDLn7qmV9",
        "outputId": "4936ef18-e846-49b3-cdf7-c06d9af16754"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MiniVQA(\n",
            "  (lstms): LSTM(300, 512)\n",
            "  (linears): Sequential(\n",
            "    (0): Linear(in_features=8192, out_features=128, bias=True)\n",
            "    (1): Dropout(p=0.2, inplace=False)\n",
            "    (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (3): Tanh()\n",
            "    (4): Linear(in_features=128, out_features=10, bias=True)\n",
            "    (5): Tanh()\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "miniVQA = MiniVQA(embed_size , 512)\n",
        "print(miniVQA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nu-_5v-EqpVV"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJq6uVodrM5T"
      },
      "source": [
        "## Define constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRN_YyfBrPgT"
      },
      "outputs": [],
      "source": [
        "learning_rate = 3e-4\n",
        "weight_decay = 4e-5\n",
        "epochs = 15"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMpx6Jj2rOzc"
      },
      "source": [
        "## Define train loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62XXfS2QqwI8"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(miniVQA.parameters(), lr=learning_rate, weight_decay=weight_decay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "5vLNMxQG2Pnr"
      },
      "outputs": [],
      "source": [
        "def pred_val(model, dataloader):\n",
        "  size = len(dataloader.dataset)\n",
        "  correct = 0\n",
        "  avg_loss = 0\n",
        "  for batch, (text, image, y) in enumerate(dataloader):\n",
        "    pred = model(text, image)\n",
        "    loss = loss_fn(pred, y)\n",
        "    output = [torch.argmax(o).item() for o in pred]\n",
        "    correct += (torch.FloatTensor(output) == y).float().sum()\n",
        "    avg_loss += loss.item()\n",
        "  acc = correct / size\n",
        "  return avg_loss, correct, acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVftN_IQqrVL"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    correct = 0\n",
        "    avg_loss = 0\n",
        "    for batch, (text, image, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(text, image)\n",
        "        loss = loss_fn(pred, y)\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward(retain_graph=True)\n",
        "        optimizer.step()\n",
        "\n",
        "        output = [torch.argmax(o).item() for o in pred]\n",
        "        correct += (torch.FloatTensor(output) == y).float().sum()\n",
        "        avg_loss += loss.item()\n",
        "\n",
        "    avg_loss /= (size // 64 + 1)\n",
        "    acc = correct / size\n",
        "    val_loss, val_correct, val_acc = pred_val(miniVQA, valid_dataloader)\n",
        "    print(f\"training / loss: {avg_loss:>7f} | accuracy: {acc}\")\n",
        "    print(f\"val / loss: {val_loss:>7f} | accuracy: {val_acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whhpnpkWqz-I",
        "outputId": "8466b812-2438-491b-e665-36251ab84aa9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "training / loss: 2.286499 | accuracy: 0.19871795177459717\n",
            "val / loss: 4.557486 | accuracy: 0.2545454502105713\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "training / loss: 2.268170 | accuracy: 0.31282052397727966\n",
            "val / loss: 4.528966 | accuracy: 0.33636364340782166\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "training / loss: 2.251997 | accuracy: 0.4115384519100189\n",
            "val / loss: 4.498886 | accuracy: 0.44545453786849976\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "training / loss: 2.238927 | accuracy: 0.44999998807907104\n",
            "val / loss: 4.469762 | accuracy: 0.4727272689342499\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "training / loss: 2.221536 | accuracy: 0.550000011920929\n",
            "val / loss: 4.442209 | accuracy: 0.4727272689342499\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "training / loss: 2.205818 | accuracy: 0.6128205060958862\n",
            "val / loss: 4.416873 | accuracy: 0.6181818246841431\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "training / loss: 2.197567 | accuracy: 0.6769230961799622\n",
            "val / loss: 4.394272 | accuracy: 0.6181818246841431\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "training / loss: 2.179637 | accuracy: 0.7333333492279053\n",
            "val / loss: 4.372013 | accuracy: 0.6272727251052856\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "training / loss: 2.171420 | accuracy: 0.75\n",
            "val / loss: 4.364040 | accuracy: 0.6454545259475708\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "training / loss: 2.161596 | accuracy: 0.7833333611488342\n",
            "val / loss: 4.338339 | accuracy: 0.7363636493682861\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "training / loss: 2.153098 | accuracy: 0.8115384578704834\n",
            "val / loss: 4.323297 | accuracy: 0.7545454502105713\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "training / loss: 2.145172 | accuracy: 0.8192307949066162\n",
            "val / loss: 4.317935 | accuracy: 0.7181817889213562\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "training / loss: 2.135622 | accuracy: 0.8371794819831848\n",
            "val / loss: 4.298903 | accuracy: 0.7636363506317139\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "training / loss: 2.129930 | accuracy: 0.8692307472229004\n",
            "val / loss: 4.278972 | accuracy: 0.7454545497894287\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "training / loss: 2.120935 | accuracy: 0.8602564334869385\n",
            "val / loss: 4.276580 | accuracy: 0.7363636493682861\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, miniVQA, loss_fn, optimizer)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "442WsPck8mcR"
      },
      "source": [
        "# Predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YG7AHv7V8nkW",
        "outputId": "ed832b8a-814a-4c68-97b4-506eaafb7d05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   question_id  label\n",
            "0       144000      1\n",
            "1       436017      1\n",
            "2       706000      8\n",
            "3      1497002      8\n",
            "4      1518004      2\n"
          ]
        }
      ],
      "source": [
        "pred = miniVQA(test_q_embeddings, test_img)\n",
        "output = np.array([torch.argmax(o).item() for o in pred], dtype='int64')\n",
        "df = pd.DataFrame({\n",
        "    'question_id': sorted(test_csv.index.values),\n",
        "    'label': output\n",
        "})\n",
        "print(df.head())\n",
        "df.to_csv(base_path + '/minivqa1-submission.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNp5dCja6oz2"
      },
      "source": [
        "# Save model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "LdG_ldAI6rwL"
      },
      "outputs": [],
      "source": [
        "torch.save(miniVQA.state_dict(), base_path + 'minivqa1_weights.pth')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "MxtUB_kckUqZ"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
